{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_maximization(D, k, epsilon, no_iterations):\n",
    "\n",
    "    # finding no of rows and no of columns without classifier in data \n",
    "    n = len(D)\n",
    "    d = len(D[0])\n",
    "\n",
    "    # Making empty list of all the data required for analysis probability means and no of iterations\n",
    "    probabilities = []\n",
    "    means = []\n",
    "    iterations = []\n",
    "\n",
    "    # for itertating 20 times\n",
    "    for iteration in range(no_iterations):\n",
    "\n",
    "        # Doing all initialising in every iteration\n",
    "        # Making an empty posterior_probability array list of dimension(no_of_clusters(k) * no_of_rows of data(n))\n",
    "        posterior_probability = np.empty((k, n))\n",
    "\n",
    "        # Taking k different mean which is k random points from Data\n",
    "        mean = np.array([np.array(D[np.random.randint(0, n)]) for i in range(k)])\n",
    "\n",
    "        # Making an Empty array of the same dimension as mean\n",
    "        new_mean = np.empty((k, d))\n",
    "\n",
    "        # Making a list of k priors initially taking 1/k\n",
    "        priors = np.array(np.full(k, 1 / k))\n",
    "\n",
    "        # Initially setting error to 1\n",
    "        error = 1\n",
    "\n",
    "        # Initialising initial covariance matrix as identity matrix\n",
    "        cov_mat = np.array([np.identity(d) for i in range(k)])\n",
    "\n",
    "        # Intialisting t as 0 its number of iteration data set takes to converge\n",
    "        t = 0\n",
    "        # Running while loop till error become less than epsilon\n",
    "        while error >= epsilon:\n",
    "\n",
    "            # increasing t every time \n",
    "            t += 1\n",
    "\n",
    "            # Calculation probability distribution for each cluster and setting it in posterier probability\n",
    "            for cluster in range(k):\n",
    "                pdf = multivariate_normal.pdf(\n",
    "                    D,\n",
    "                    mean=mean[cluster],\n",
    "                    cov=cov_mat[cluster],\n",
    "                    allow_singular = True\n",
    "                )\n",
    "                posterior_probability[cluster] = np.matmul(pdf.reshape(n, 1), priors[cluster].reshape(1))\n",
    "\n",
    "            # Calculating Total sum of posterier probability for each point \n",
    "            # and then dividing the posterier probability in each cluster by it\n",
    "            for row in range(n):\n",
    "                total = 0\n",
    "                for cluster in range(k):\n",
    "                    total += posterior_probability[cluster][row]\n",
    "                for cluster in range(k):\n",
    "                    posterior_probability[cluster][row] = posterior_probability[cluster][row] / total\n",
    "\n",
    "            # Iterating through each cluster to reestimate mean, covariance matrix and priors\n",
    "            for cluster in range(k):\n",
    "\n",
    "                # Calculating total_posterior_probability in ith cluster\n",
    "                total_posterior_probability = np.sum(posterior_probability[cluster]) + epsilon\n",
    "\n",
    "                # Caculating new mean for ith cluster by multiplying each column with posterior probability \n",
    "                # and dividing it by total_posterior_probability in ith cluster \n",
    "                new_mean[cluster] = np.matmul(posterior_probability[cluster].reshape(1, n), D) / total_posterior_probability\n",
    "\n",
    "                # Calculating the ith prior diving total_posterior_probability by total number of rows\n",
    "                priors[cluster] = total_posterior_probability / n\n",
    "\n",
    "                # Making an empty numerator array for calculating new covariance for particular cluster\n",
    "                numerator = np.zeros((d, d))\n",
    "\n",
    "                # Iterating through each row finding it difference from new mean and multiplting it by its transpose\n",
    "                # and add summing up all numberators and then dividing it by total_posterior_probability in ith cluster\n",
    "                for row in range(n):\n",
    "                    difference = np.subtract(D[row], new_mean[cluster]).reshape((d, 1))\n",
    "                    mul = posterior_probability[cluster][row] * np.matmul(difference, np.transpose(difference))\n",
    "                    numerator += mul\n",
    "                cov_mat[cluster] = numerator / total_posterior_probability\n",
    "            \n",
    "            # Calculating error by finding mean square in new and old mean\n",
    "            error = mean_squared_error(mean, new_mean)\n",
    "\n",
    "            # Making mean as new mean by making deep copy\n",
    "            mean = copy.deepcopy(new_mean)\n",
    "            \n",
    "        # apeending probability, means and iterations to list\n",
    "        probabilities.append(posterior_probability)\n",
    "        means.append(mean)\n",
    "        iterations.append(t)\n",
    "        \n",
    "    # returning all the list\n",
    "    return (means, probabilities, iterations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # reading Ionosphere data which is without any header so setting as None \n",
    "    ionosphere_df = pd.read_csv('ionosphere.data', header = None)\n",
    "    \n",
    "    # Converting data frame as numpy array by removing last columns of data frame\n",
    "    D = ionosphere_df.iloc[:, :-1].to_numpy()\n",
    "    \n",
    "    # Finding actual mean point of each category in data set to find error\n",
    "    true_mean = np.array([list(ionosphere_df[ionosphere_df[34] == 'g'].mean()), list(ionosphere_df[ionosphere_df[34] == 'b'].mean())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising errors list which will be multi-dimensional list for storing error percent for each cluster\n",
    "errors_cluster = []\n",
    "\n",
    "# Initialising iteration list which will be multi-dimensional list for storing no of iteration for each cluster\n",
    "iteration_cluster = []\n",
    "\n",
    "# list of no of clusters\n",
    "clusters = [2, 3, 4, 5]\n",
    "\n",
    "# Iterating through each cluster and passing it into function\n",
    "for cluster in clusters:\n",
    "    (means, probabilities, iterations) = expectation_maximization(D, cluster, 0.00001, 20)\n",
    "    errors = []\n",
    "    \n",
    "    # finding prediction of cluster depending upon posterior probability of each point in each cluster\n",
    "    predictions = [np.argmax(posterior_probability, axis=0) for posterior_probability in probabilities]\n",
    "    \n",
    "    # going through mean for each iteration in list \n",
    "    for j in range(len(means)):\n",
    "        \n",
    "        # initialising error as 0\n",
    "        error = 0\n",
    "        \n",
    "        # setting mean as mean points of specific iteration\n",
    "        mean = means[j]\n",
    "        \n",
    "        # Iterating through each mean point\n",
    "        for i in range(len(mean)):\n",
    "            \n",
    "            # Calculating the eucledian distance of mean points wrt to true mean points\n",
    "            temp = true_mean[0] - mean[i]\n",
    "            sum_sq = np.dot(temp.T, temp)\n",
    "            eucl1 = np.sqrt(sum_sq)\n",
    "            temp = true_mean[1] - mean[i]\n",
    "            sum_sq = np.dot(temp.T, temp)\n",
    "            eucl2 = np.sqrt(sum_sq)\n",
    "            \n",
    "            # findinding index of point in this particular cluster\n",
    "            indices = [index for index, element in enumerate(predictions[j]) if element == i]\n",
    "            \n",
    "            # depending upon eucledian distance setting cluster value\n",
    "            cluster = 'b'\n",
    "            if eucl1 < eucl2:\n",
    "                cluster = 'g'\n",
    "                \n",
    "            # going through each index checking predicted and actual cluster are same, if not increasing error \n",
    "            for index in indices:\n",
    "                if ionosphere_df.iloc[index,-1] != cluster:\n",
    "                    error += 1\n",
    "                    \n",
    "        # Appending error to the list\n",
    "        errors.append(error * 100/ len(D))\n",
    "        \n",
    "    # appending whole error to error_cluster\n",
    "    errors_cluster.append(errors)\n",
    "    \n",
    "    # Appending no of iterations to list\n",
    "    iteration_cluster.append(iterations)\n",
    "print(errors_cluster)\n",
    "print(len(errors_cluster[0]))\n",
    "print(np.asarray(iteration_cluster).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1:\n",
    "\n",
    "We can find the longest prefix of P that is a substring of D using KMP. \n",
    "Here, we will compute a fail table to store patterns of repeating occurences. \n",
    "So, we can say that there was a match for first k-1 elements of the pattern and the string whenever there is mismatch between the pattern and string at P[k]. Here, what we can do is store the values in a array till we get a mismatch (i.e k-1 letters will be matched if we get mismatch at kth letter and we will store these k letter in an array). When we are finished calculating mismatches we wil return the longest array. And this array will be the ans because that array will be the longest substring of D which is present in P.\n",
    "\n",
    "Time complexity of this algorithm will be O(len(D) + len(P)):\n",
    "where, time complexity of len(P) will be required for itereating through D and len(p) will be required to compute fail table.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 \n",
      "1 2 3 4 \n",
      "1 2 3 4 \n",
      "3\n",
      "1\n",
      "(1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "def binarysearch(arr, low, high, key):\n",
    "    if low < high:\n",
    "        mid = low + (high - low) // 2\n",
    "        if arr[mid] == key:\n",
    "            return True\n",
    "        elif arr[mid] < key:\n",
    "            return binarysearch(arr, mid + 1, high, key)\n",
    "        else:\n",
    "            return binarysearch(arr, low, mid - 1, key)\n",
    "    return False\n",
    "\n",
    "def solution(a, b, c, t):\n",
    "    a.sort()\n",
    "    b.sort()\n",
    "    c.sort()\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            to_find = t - (i + j)\n",
    "            print(to_find)\n",
    "            if binarysearch(c, 0, len(c), to_find):\n",
    "                return i, j, to_find\n",
    "    return False\n",
    "\n",
    "\n",
    "a = list(map(int, input().split()))\n",
    "b = list(map(int, input().split()))\n",
    "c = list(map(int, input().split()))\n",
    "t = int(input())\n",
    "print(solution(a, b, c, t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.3\n",
    "\n",
    "Input: Compressed trie T and a string S\n",
    "Output:T without string S\n",
    "\n",
    "delete_string(T, S):\n",
    "    Search compressed trie T \n",
    "    Find string S in it\n",
    "    if s is not found in compresssed trie: \n",
    "        return False\n",
    "    else:\n",
    "        let k be the node where we have found string S \n",
    "        if k has child:  return False\n",
    "        else:\n",
    "            let Pk be the parent of k \n",
    "            delete k \n",
    "        if Pk has a single child l remaing after deletion:\n",
    "            Pk.string ← Pk.string + l.string\n",
    "            delete node l\n",
    "            return True \n",
    "                \n",
    " \n",
    "For searching the string we will require O(len(S)) linear time\n",
    "To delete the string from the compressed trie we will require O(1) constant time\n",
    "Total time complexity of this algorithm is O(len(S) linear time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
